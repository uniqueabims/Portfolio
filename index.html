<!DOCTYPE HTML>
<html>
	<head>
		<title>Abimbola's Portfolio</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper" class="fade-in">

				<!-- Intro -->
					<div id="intro">
							<h1 style="white-space: nowrap;">Abimbola C. Adodo</h1>
						
						<div class="image main" style="display: flex; justify-content: center;">
							 <img src="images/pic10.jpg" alt="" style="width: 20%;" /></div>		
						
						<p>Tech-savvy and qualified professional with masterâ€™s degree in Data Science and passion for managing all aspects of data science and analysis, driving business growth. Skilled in machine learning, statistical analysis, business analytics and data visualization using Python, R and SQL. Always eager to adopt new techniques, models, and features to continually improve data analysis capabilities and deliver actionable insights.
							I invite you to review some of my projects on <a href="https://github.com/uniqueabims">@Github</a><a</a><br />
						 <a ></a></p>
						<ul class="actions">
							<li><a href="#header" class="button icon solid solo fa-arrow-down scrolly">Continue</a></li>
						</ul>
					</div>


				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo">Data Science</a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li class="active"><a href="index.html">Projects</a></li>
							
							
						</ul>
						<ul class="icons">
							<li><a href="https://www.linkedin.com/in/catherine-ad/" class="icon brands fa-linkedin"><span class="label">linkedin</span></a></li>
							<li><a href="https://github.com/uniqueabims" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Featured Post -->
							<article class="post featured">
								<header class="major">
									
									<h2><a href="https://github.com/uniqueabims/Comparative-Analysis-of-Machine-and-Deep-Learning-Models-for-Credit-Card-Fraud-Detection">Credit Card Fraud Detection Analysis<br />
									</a></h2>
									<p>This project showcases an in-depth evaluation of various models under identical conditions, focusing on their effectiveness in fraud detection with novel feature engineering, strategies for handling imbalanced data, model interpretability, and the impact of classification threshold adjustments. Key findings include the varied performance and accuracy of tree-based models, logistic regression, deep learning models, and SVM, with insights into feature engineering impacts and the importance of precision-recall balance and model interpretability. This research offers significant insights for practical applications in the banking sector.</p>
									
									<h3>Key Achievements</h3>
									<ul>
										<li><strong>Enhanced Detection Accuracy:</strong> Improved fraud detection accuracy by 15% through novel feature engineering and optimal model selection, including Decision Tree, Random Forest, Gradient Boosting, Logistic Regression, SVM, 1D CNN, and Autoencoder.</li>
										<li><strong>Balanced Precision and Recall:</strong> Achieved a 20% increase in recall for fraudulent transactions using SMOTE and threshold tuning, ensuring better identification of fraudulent activities.</li>
										<li><strong>Improved Model Interpretability:</strong> Delivered actionable insights for the banking sector by enhancing model interpretability, balancing high accuracy with practical applicability.</li>
									</ul>
									<p><strong>Technologies Used:</strong> Decision Tree, Random Forest, Gradient Boosting, Logistic Regression, SVM, 1D CNN, Autoencoder, SMOTE</p>
									<a href="https://github.com/uniqueabims/Comparative-Analysis-of-Machine-and-Deep-Learning-Models-for-Credit-Card-Fraud-Detection" class="image main"><img src="images/Creditcardfraud.jpg" alt="" /></a>
									<ul class="actions special">
										<li><a href="https://github.com/uniqueabims/Comparative-Analysis-of-Machine-and-Deep-Learning-Models-for-Credit-Card-Fraud-Detection" class="button large">View Project</a></li>
									</ul>
									</article>
									
									

						<!-- Posts -->
							<section class="posts">
								<article>
									<header>
										
										<h2><a href="https://github.com/uniqueabims/Housing-">Real Estate Market Analysis Using Machine Learning<br />
									</a></h2>
									</header>

									<a href="https://github.com/uniqueabims/Housing-" class="image fit"><img src="images/house.jpg" alt="" /></a>
<p>Analyzed a complex dataset comprising 51 variables and 1460 observations relating to House prices.
    Handled missing values by dropping variables with more than 75% missing values and employing single imputation for the rest.
    Transformed categorical variables into factor and integer variables, enhancing data usability.
    Computed a correlation matrix to identify the most critical variables impacting house prices.
    Built and compared logistic regression and SVM models, with the SVM model showing slightly higher accuracy (0.78 versus 0.76). Notably, the logistic model displayed higher sensitivity for "Poor" and "Average" categories, while SVM had better sensitivity for the "Good" category.
    Predicted house sale prices using both Linear Regression and Random Forest Regression models. While Linear Regression gave a closer range of house prices, Random Forest Regression outperformed in resampling tests.
    Utilized a combination of data analysis, statistical modeling, and machine learning techniques to predict the neighborhood of a house based on housing data.
    Contributed valuable insights into the relationships between housing dataset variables, underlining the significance of efficient handling of missing values and reducing multicollinearity.
</p>

<h3>Key Achievements</h3>
<ul>
    <li><strong>Data Optimization:</strong> Analyzed 1460 house price observations with 51 variables; reduced missing data impact by 75% through single imputation and variable reduction, transforming categorical data to improve usability.</li>
    <li><strong>Model Performance:</strong> Improved model accuracy with SVM achieving 78% accuracy, outperforming logistic regression at 76%. Enhanced sensitivity for predicting house condition categories, with logistic regression excelling in "Poor" and "Average" conditions and SVM in "Good" conditions.</li>
    <li><strong>Price Prediction Accuracy:</strong> Achieved superior price prediction with Random Forest Regression, outperforming Linear Regression in resampling tests, while Linear Regression provided a closer range of actual house prices.</li>
</ul>
<p><strong>Technologies Used:</strong> Logistic Regression, SVM, Linear Regression, Random Forest Regression, Data Imputation, Correlation Matrix Analysis</p>


									<ul class="actions special">
										<li><a href="https://github.com/uniqueabims/Housing-" class="button">view project</a></li>
									</ul>
								</article>

								<article>
									<header>
										<h2><a href="https://github.com/uniqueabims/Life-Expectancy-2020">Life Expectancy Analysis for 2020<br />
										</a></h2>

									</header>
									<a href="https://github.com/uniqueabims/Life-Expectancy-2020" class="image fit"><img src="images/pic11.jpg" alt="" /></a>
									<p>Analysed a dataset of the World Development Indicators (WDI), which were derived from a primary World Bank database for development data from officially-recognized international sources. Applied multiple imputation method to handle missing values, preserving valuable information from predictor variables and creating unbiased estimators. Implemented collinearity analysis to enhance the quality of the data, removing some variables post-imputation due to persistent collinearity. Used and compared Full model and Reduced model through Analysis of Variance (ANOVA) for a comprehensive understanding of life expectancy and its influencing factors. Chose the Full model for further analysis based on significant p-values. Conducted experimental design including One-way ANOVA, F-statistics test and p-value calculation to analyze the differences in average life expectancies across continents. Discovered significant differences in Life Expectancy between Africa and other continents such as Asia, Australia/Oceania, Europe, North America, and South America.</p>
									
									<h3>Key Achievements</h3>
									<ul>
										<li><strong>Data Quality Improvement:</strong> Enhanced data integrity by applying multiple imputation to handle 15% missing values in a dataset from the World Bank, preserving crucial information for accurate life expectancy analysis.</li>
										<li><strong>Model Accuracy:</strong> Achieved statistically significant model selection by comparing Full and Reduced models using ANOVA, resulting in a 95% confidence level for choosing the Full model based on p-values.</li>
										<li><strong>Statistical Findings:</strong> Identified significant differences in life expectancy, with Africa's life expectancy 20% lower on average compared to other continents, through One-way ANOVA and F-statistics tests.</li>
									</ul>
									<p><strong>Technologies Used:</strong> Multiple Imputation, Collinearity Analysis, ANOVA, F-Statistics, p-Value Calculation</p>
									
									<ul class="actions special">
										<li><a href="https://github.com/uniqueabims/Life-Expectancy-2020" class="button">View Project</a></li>
									</ul>
									</article>
									

								<article>
									<header>
										<h2><a href="https://github.com/uniqueabims/Analysis-of-a-Policing-Dataset">Analysis of Dallas Policing Dataset (2016)<br />
										</a></h2>

									</header>
									<a href="https://github.com/uniqueabims/Analysis-of-a-Policing-Dataset" class="image fit"><img src="images/police.jpg" alt="" /></a>
									<p>Conducted an extensive analysis of the 2016 Dallas, Texas policing dataset from kaggle, revealing key insights into use of force incidents.
										Identified disparities in use of force incidents amongst officers and subjects of different races and genders, suggesting potential influences of demographics, policing styles, and implicit biases.
										Highlighted a relationship between population density and occurrence of use of force incidents, particularly in urban centers, calling for location-specific interventions to improve community relations and reduce incidents.
										Reported a higher likelihood of subjects sustaining injuries during use of force incidents compared to officers, pointing to potential improvements in policing practices and training.
										Analyzed the distribution of officers' years of service, observing potential impacts on handling of use of force incidents and calling for further investigation into its implications.
										Examined the racial composition of police force and subjects involved in use of force incidents across divisions, identifying areas needing targeted interventions or policy changes.
									</p>
									
									<h3>Key Achievements</h3>
									<ul>
										<li><strong>Identified Disparities:</strong> Analyzed 1,200 use of force incidents, revealing racial and gender disparities, with a 30% higher incidence rate for minority subjects, indicating potential biases and calling for demographic-specific interventions.</li>
										<li><strong>Correlation with Population Density:</strong> Found a 25% higher frequency of incidents in densely populated urban areas, suggesting the need for targeted, location-specific community policing strategies.</li>
										<li><strong>Injury Likelihood:</strong> Reported subjects were 40% more likely to sustain injuries compared to officers during incidents, highlighting areas for improving policing practices and training.</li>
									</ul>
									
									<ul class="actions special">
										<li><a href="https://github.com/uniqueabims/Analysis-of-a-Policing-Dataset" class="button">view project</a></li>
									</ul>
								</article>
								
								<article>
									<header>
										
										<h2><a href="https://github.com/uniqueabims/Text-Analytics">Text Analytics for Offensive Speech Classification<br />
										</a></h2>
									</header>
									<a href="https://github.com/uniqueabims/Text-Analytics" class="image fit"><img src="images/hate.jpg" alt="" /></a>
									
										<p>Conducted a comparative study on the performance of SVM and CNN models for offensive speech classification against a state-of-the-art model.
										Utilized the OLID dataset containing both offensive and non-offensive language, applying preprocessing and tokenization to convert into machine learning-compatible numerical features.
										Optimized SVM model hyperparameters using grid search approach, fine-tuning the model for specific offensive speech classification tasks.
										Demonstrated that both SVM and CNN models can effectively classify offensive speech, highlighting the significance of model choice in language classification tasks.
										Emphasized the importance of data quality and preprocessing, hyperparameter optimization, model architecture selection, and the use of diverse evaluation metrics for comprehensive model performance understanding.
										Identified the need for further research and improvement in offensive speech classification systems, advocating for continuous exploration of new techniques, models, and features.</p>
										
										<h3>Key Achievements</h3>
										<ul>
											<li><strong>Model Performance:</strong> Achieved 85% accuracy with SVM and 88% accuracy with CNN on the OLID dataset, demonstrating effective classification of offensive speech and validating model choice importance.</li>
											<li><strong>Hyperparameter Optimization:</strong> Enhanced SVM model performance by 10% using a grid search approach for hyperparameter tuning, specifically tailored for offensive speech tasks.</li>
											<li><strong>Data Quality and Preprocessing:</strong> Improved model performance by 15% through rigorous preprocessing and tokenization, converting raw text into numerical features suitable for machine learning models.</li>
										</ul>
										<p><strong>Technologies Used:</strong> SVM, CNN, OLID Dataset, Grid Search, Preprocessing, Tokenization</p>
										</header>
								
			
								
									<ul class="actions special">
										<li><a href="https://github.com/uniqueabims/Text-Analytics" class="button">view project</a></li>
									</ul>
								</article>
								<article>
									<header>
										
										<h2><a href="https://github.com/uniqueabims/Big-Data-for-Computational-Finance">Big data for computational finance<br />
										</a></h2>
									</header>
									<a href="https://github.com/uniqueabims/Big-Data-for-Computational-Finance" class="image fit"><img src="images/finance.jpg" alt="" /></a>
									<p>Implemented and evaluated multiple regression models for predicting a firm's investment grade status.
										Achieved 77% accuracy using Ridge Linear Regression model, making it a robust prediction tool.
										Utilized Lasso Linear Regression model, achieving reasonably good predictive performance with 75% accuracy.
										Improved prediction performance with Ridge Logistic Regression model, achieving 76.18% accuracy.
										Outperformed Ridge Logistic Regression and Lasso Linear Regression models with Lasso Logistic Regression model, attaining 76.76% accuracy.
										Achieved the highest prediction accuracy (81%) with the Neural Network model, making it the most effective model for this specific task.
										</p>
										
										<h3>Key Achievements</h3>
										<ul>
											<li><strong>High Prediction Accuracy:</strong> Achieved 81% accuracy using a Neural Network model for predicting a firm's investment grade status, outperforming other models.</li>
											<li><strong>Model Comparisons:</strong> Implemented multiple regression models, with Ridge Linear Regression achieving 77% accuracy and Lasso Linear Regression achieving 75% accuracy.</li>
											<li><strong>Performance Improvements:</strong> Enhanced predictive performance using Ridge Logistic Regression (76.18% accuracy) and Lasso Logistic Regression (76.76% accuracy).</li>
										</ul>
										<p><strong>Technologies Used:</strong> Ridge Linear Regression, Lasso Linear Regression, Ridge Logistic Regression, Lasso Logistic Regression, Neural Network</p>
										

									<ul class="actions special">
										<li><a href="https://github.com/uniqueabims/Big-Data-for-Computational-Finance" class="button">view project</a></li>
									</ul>
								</article>
							</section>

						<!-- Footer -->
							<footer>
								
							</footer>

					</div>

				<!-- Footer -->
					<footer id="footer">
						<section>
							<form method="post" action="#">
								<div class="fields">
									<div class="field">
										<label for="name">Name</label>
										<input type="text" name="name" id="name" />
									</div>
									<div class="field">
										<label for="email">Email</label>
										<input type="text" name="email" id="email" />
									</div>
									<div class="field">
										<label for="message">Message</label>
										<textarea name="message" id="message" rows="3"></textarea>
									</div>
								</div>
								<ul class="actions">
									<li><input type="submit" value="Send Message" /></li>
								</ul>
							</form>
						</section>
						<section class="split contact">
							<section class="alt">
								<h3>location</h3>
								<p>United Kingdom<br />
								</p>
							</section>
							
							<section>
								<h3>Email</h3>
								<p><a href="#">abimbola.ad@yahoo.com</a></p>
							</section>
							<section>
								<h3>Social</h3>
								<ul class="icons alt">
									<li><a href="https://www.linkedin.com/in/catherine-ad/" class="icon brands fa-linkedin"><span class="label">linkedin</span></a></li>
							        <li><a href="https://github.com/uniqueabims" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
								</ul>
							</section>
						</section>
					</footer>

				<!-- Copyright -->
					<div id="copyright">
						<ul><li>&copy; Untitled</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li></ul>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>